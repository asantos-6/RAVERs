{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAVE: Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import base64\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fiftyone.brain as fob\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import rave\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_path(absolute_path):\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "def find_wav_files(folder_path):\n",
    "    wav_files = []\n",
    "    for root, _, _ in os.walk(folder_path):\n",
    "        wav_files.extend(glob.glob(os.path.join(root, '*.wav')))\n",
    "    return wav_files\n",
    "\n",
    "def trim_audio(audio, sr, ti, tf, mono=True):\n",
    "    i = ti * sr\n",
    "    f = tf * sr\n",
    "    if mono: t_audio = audio[i:f]\n",
    "    else: t_audio = audio[:, i:f]\n",
    "    return t_audio\n",
    "\n",
    "def read_audio(file_path, trim_interval=None, mono=True, print_it=False):\n",
    "    audio, sr = librosa.load(file_path, mono=mono)\n",
    "    audio_dim = len(audio.shape)\n",
    "    if not mono and audio_dim == 1:\n",
    "        audio = np.asarray((audio, audio))\n",
    "    if trim_interval is not None:\n",
    "        ti = trim_interval[0]\n",
    "        tf = trim_interval[1]\n",
    "        audio = trim_audio(audio, sr, ti, tf, mono)\n",
    "    if print_it:\n",
    "        print(audio.shape)\n",
    "        print(sr)\n",
    "    return audio, sr\n",
    "\n",
    "def remix_audio(left_audio_array, right_audio_array, sample_rate=44100):\n",
    "    # ensure both audio arrays have the same length\n",
    "    length = min(len(left_audio_array), len(right_audio_array))\n",
    "    left_audio_array = left_audio_array[:length]\n",
    "    right_audio_array = right_audio_array[:length]\n",
    "    stereo_audio_array = np.column_stack((left_audio_array, right_audio_array)) # create stereo array\n",
    "    return stereo_audio_array\n",
    "\n",
    "def remove_common_part(file_names):\n",
    "    common_prefix = os.path.commonprefix(file_names)\n",
    "    # common_suffix = os.path.commonprefix([name[::-1] for name in file_names])[::-1]\n",
    "    return [name[len(common_prefix):] for name in file_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'drumkit_v1'\n",
    "model_name = 'GMDrums_v3_29-09_3M_streaming'\n",
    "model = torch.jit.load(f'../models/{model_name}.ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_folder = '../data/WAV/Individual Hits'\n",
    "#sample_folder = '../data/finger_tapping'\n",
    "samples = find_wav_files(sample_folder)\n",
    "\n",
    "print(f'Found a total of {len(samples)} samples:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_latents = []\n",
    "samples_filtered = []\n",
    "\n",
    "for sample_path in samples:\n",
    "    min_length = 4410\n",
    "    desired_length = 88200\n",
    "    sample_audio, wav_sr = read_audio(sample_path)\n",
    "    if sample_audio.shape[0] < min_length: continue # ignore files shorter than 100ms\n",
    "    samples_filtered.append(sample_path) # add to list of filtered files\n",
    "    padding_width = max(0, desired_length - len(sample_audio)) # calculate padding width\n",
    "    sample_audio = np.pad(sample_audio, (0, padding_width), mode='constant', constant_values=0) # pad with zeros\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(sample_audio).reshape(1, 1, -1)\n",
    "        z = model.encode(x) # encode the audio into the RAVE latent space\n",
    "        latent_space_matrix = torch.squeeze(z, 0)\n",
    "        sample_latents.append(latent_space_matrix) # add to list of latent space matrices\n",
    "\n",
    "# make sure all tensors have the same shape\n",
    "tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "if all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    print('All tensors have same shape.')\n",
    "else:\n",
    "    tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "    different_dimension_index = np.where(np.array(tensor_shapes) != tensor_shapes[0])[0]\n",
    "    for index in different_dimension_index:\n",
    "        print('There were some tensors that happen to have different shape:')\n",
    "        print(f'Removing sample {samples_filtered[index]} and its corresponding latent space matrix.')\n",
    "        del samples_filtered[index]\n",
    "        del sample_latents[index]\n",
    "        del tensor_shapes[index]\n",
    "\n",
    "sample_latents_np = np.array(sample_latents) # convert sample_latents to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_tensors = [tensor.flatten() for tensor in sample_latents]\n",
    "embeddings = np.vstack(flattened_tensors)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use t-SNE in order to narrow down to 2 the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = samples_filtered\n",
    "audio_names = remove_common_part(samples_filtered)\n",
    "\n",
    "dataset = fo.Dataset() # create a FiftyOne SampleCollection\n",
    "\n",
    "# create a sample for each audio file\n",
    "for audio_path, audio_name in zip(audio_paths, audio_names):\n",
    "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    if 'BD' in audio_name: label = 'KD' # label BD\n",
    "    elif 'SD' in audio_name: label = 'SD' # label SD\n",
    "    else: label = 'other' # label other\n",
    "    # create and append sample\n",
    "    sample = fo.Sample(filepath=audio_path, audio=audio_data, sample_rate=sample_rate, label=label, audio_name=audio_name)\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# dataset.save('../data') # save the the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a 2D representation using t-SNE\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings,\n",
    "    num_dims=2,\n",
    "    method='tsne',\n",
    "    brain_key='mnist_test',\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results.points, columns=['x', 'y'])\n",
    "df['label'] = dataset.values('label')\n",
    "df['audio_name'] = dataset.values('audio_name')\n",
    "df['filepath'] = dataset.values('filepath')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot of the latent space\n",
    "data = []\n",
    "for label in df['label'].unique():\n",
    "    df_label = df[df['label'] == label]\n",
    "    scatter = go.Scatter(\n",
    "        x=df_label['x'], \n",
    "        y=df_label['y'], \n",
    "        mode='markers',\n",
    "        text=df_label['audio_name'],  # Add the audio name as text\n",
    "        hovertemplate='%{text}<extra></extra>',  # Customize the hover template\n",
    "        name=label,  # Use the label as the name of the trace\n",
    "        customdata=df_label['filepath']  # Add the file path as custom data\n",
    "    )\n",
    "    scatter.on_click(lambda x: print(x.points[0].hovertext))  # Print the name of the point when it's clicked\n",
    "    data.append(scatter)\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Adjust the margins (l, r, t, b stand for left, right, top, and bottom)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    margin=dict(l=24,r=24,b=24,t=24,pad=0)\n",
    ")\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id='scatter-plot', figure=fig),\n",
    "    html.Pre(id='click-data', style={'padding': '10px', 'color': 'white'}),\n",
    "    html.Audio(id='input-player', controls=True, autoPlay=True),\n",
    "    html.Audio(id='output-player', controls=True, autoPlay=False)\n",
    "])\n",
    "\n",
    "@app.callback(Output('click-data', 'children'), [Input('scatter-plot', 'clickData')])\n",
    "def display_click_data(clickData):\n",
    "    if clickData is None: return 'None'\n",
    "    absolute_path = clickData[\"points\"][0][\"customdata\"]\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "@app.callback(Output('input-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_input(clickData):\n",
    "    if clickData is None: return ''\n",
    "\n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "    with open(relative_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "    \n",
    "    return src\n",
    "\n",
    "@app.callback(Output('output-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_output(clickData):\n",
    "    if clickData is None: return ''\n",
    "    \n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "\n",
    "    audio, sr = read_audio(relative_path)\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(audio).reshape(1 ,1, -1)\n",
    "        z = model.encode(x)\n",
    "        x_hat = model.decode(z)\n",
    "    waveform_tensor = torch.squeeze(x_hat, 0)\n",
    "    output_path = '../output/output.wav'\n",
    "    torchaudio.save(output_path, waveform_tensor, sr)\n",
    "\n",
    "    with open(output_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "\n",
    "    return src\n",
    "\n",
    "if __name__ == '__main__': app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation between two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointA = '/home/asantos-6/Documents/PhD/code/RAVERs/data/finger_tapping/onset_1.wav'\n",
    "pointB = '/home/asantos-6/Documents/PhD/code/RAVERs/data/WAV/Individual Hits/02. Snare Drum/01. Clean/SD 909 Clean B 09.wav'\n",
    "\n",
    "samples = [pointA, pointB]\n",
    "\n",
    "model_name = 'GMDrums_v3_29-09_3M_streaming'\n",
    "model = torch.jit.load(f'../models/{model_name}.ts')\n",
    "\n",
    "sample_latents = []\n",
    "samples_filtered = []\n",
    "\n",
    "for sample_path in samples:\n",
    "    min_length = 4410\n",
    "    desired_length = 44100\n",
    "    sample_audio, wav_sr = read_audio(sample_path)\n",
    "    if sample_audio.shape[0] < min_length: continue # ignore files shorter than 100ms\n",
    "    samples_filtered.append(sample_path) # add to list of filtered files\n",
    "    padding_width = max(0, desired_length - len(sample_audio)) # calculate padding width\n",
    "    sample_audio = np.pad(sample_audio, (0, padding_width), mode='constant', constant_values=0) # pad with zeros\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(sample_audio).reshape(1, 1, -1)\n",
    "        z = model.encode(x) # encode the audio into the RAVE latent space\n",
    "        latent_space_matrix = torch.squeeze(z, 0)\n",
    "        sample_latents.append(latent_space_matrix) # add to list of latent space matrices\n",
    "\n",
    "# make sure all tensors have the same shape\n",
    "tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "if all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    print('All tensors have same shape.')\n",
    "else:\n",
    "    tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "    different_dimension_index = np.where(np.array(tensor_shapes) != tensor_shapes[0])[0]\n",
    "    for index in different_dimension_index:\n",
    "        print('There were some tensors that happen to have different shape:')\n",
    "        print(f'Removing sample {samples_filtered[index]} and its corresponding latent space matrix.')\n",
    "        del samples_filtered[index]\n",
    "        del sample_latents[index]\n",
    "        del tensor_shapes[index]\n",
    "\n",
    "sample_latents_np = np.array(sample_latents) # convert sample_latents to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display\n",
    "\n",
    "def interpolate_tensors(tensor1, tensor2, alpha):\n",
    "    return alpha * tensor1 + (1 - alpha) * tensor2\n",
    "\n",
    "sr = wav_sr\n",
    "tensorA, tensorB = sample_latents\n",
    "\n",
    "for alpha in np.linspace(0, 1, 6):\n",
    "    with torch.no_grad():\n",
    "        inter_tensor = interpolate_tensors(tensorA, tensorB, alpha)\n",
    "        z = torch.unsqueeze(inter_tensor, 0)\n",
    "        y_hat = model.decode(z)\n",
    "        inter_audio = torch.squeeze(y_hat, 0)\n",
    "    print(f'alpha = {alpha}')\n",
    "    display(ipd.Audio(inter_audio, rate=sr, autoplay=True))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raversenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
