{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAVE: Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating database to v0.23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asantos6\\Documents\\RAVERs\\.raversenv\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import base64\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import fiftyone.brain as fob\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import rave\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_path(absolute_path):\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "def find_wav_files(*folder_paths):\n",
    "    wav_files = []\n",
    "\n",
    "    for folder_path in folder_paths:\n",
    "        if folder_path:\n",
    "            for root, _, _ in os.walk(folder_path):\n",
    "                wav_files.extend(glob.glob(os.path.join(root, '*.wav')))\n",
    "\n",
    "    return wav_files\n",
    "\n",
    "def trim_audio(audio, sr, ti, tf, mono=True):\n",
    "    i = ti * sr\n",
    "    f = tf * sr\n",
    "    if mono: t_audio = audio[i:f]\n",
    "    else: t_audio = audio[:, i:f]\n",
    "    return t_audio\n",
    "\n",
    "def read_audio(file_path, trim_interval=None, mono=True, print_it=False):\n",
    "    audio, sr = librosa.load(file_path, mono=mono)\n",
    "    audio_dim = len(audio.shape)\n",
    "    if not mono and audio_dim == 1:\n",
    "        audio = np.asarray((audio, audio))\n",
    "    if trim_interval is not None:\n",
    "        ti = trim_interval[0]\n",
    "        tf = trim_interval[1]\n",
    "        audio = trim_audio(audio, sr, ti, tf, mono)\n",
    "    if print_it:\n",
    "        print(audio.shape)\n",
    "        print(sr)\n",
    "    return audio, sr\n",
    "\n",
    "def remix_audio(left_audio_array, right_audio_array, sample_rate=44100):\n",
    "    # ensure both audio arrays have the same length\n",
    "    length = min(len(left_audio_array), len(right_audio_array))\n",
    "    left_audio_array = left_audio_array[:length]\n",
    "    right_audio_array = right_audio_array[:length]\n",
    "    stereo_audio_array = np.column_stack((left_audio_array, right_audio_array)) # create stereo array\n",
    "    return stereo_audio_array\n",
    "\n",
    "def remove_common_part(file_names):\n",
    "    common_prefix = os.path.commonprefix(file_names)\n",
    "    # common_suffix = os.path.commonprefix([name[::-1] for name in file_names])[::-1]\n",
    "    return [name[len(common_prefix):] for name in file_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'drumkit_v1'\n",
    "model_name = 'GMDrums_v3_29-09_3M_streaming'\n",
    "model_name = 'percussion'\n",
    "model_name = 'darbouka_onnx'\n",
    "model = torch.jit.load(f'../models/{model_name}.ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 1611 samples:\n"
     ]
    }
   ],
   "source": [
    "drumhits_folder = '../data/WAV/Individual Hits'\n",
    "taps_folder = '../data/finger_tapping'\n",
    "samples = find_wav_files(drumhits_folder, taps_folder)\n",
    "\n",
    "print(f'Found a total of {len(samples)} samples:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tensors have same shape.\n"
     ]
    }
   ],
   "source": [
    "sample_latents = []\n",
    "samples_filtered = []\n",
    "\n",
    "for sample_path in samples:\n",
    "    min_length = 4410\n",
    "    desired_length = 88200\n",
    "    sample_audio, wav_sr = read_audio(sample_path)\n",
    "    if sample_audio.shape[0] < min_length: continue # ignore files shorter than 100ms\n",
    "    samples_filtered.append(sample_path) # add to list of filtered files\n",
    "    padding_width = max(0, desired_length - len(sample_audio)) # calculate padding width\n",
    "    sample_audio = np.pad(sample_audio, (0, padding_width), mode='constant', constant_values=0) # pad with zeros\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(sample_audio).reshape(1, 1, -1)\n",
    "        z = model.encode(x) # encode the audio into the RAVE latent space\n",
    "        latent_space_matrix = torch.squeeze(z, 0)\n",
    "        sample_latents.append(latent_space_matrix) # add to list of latent space matrices\n",
    "\n",
    "# make sure all tensors have the same shape\n",
    "tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "if all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    print('All tensors have same shape.')\n",
    "if not all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    tensor_shapes = [tensor.shape for tensor in sample_latents]\n",
    "    different_dimension_index = np.where(np.array(tensor_shapes) != tensor_shapes[0])[0]\n",
    "    for index in different_dimension_index:\n",
    "        print('There were some tensors that happen to have different shape:')\n",
    "        print(f'Removing sample {samples_filtered[index]} and its corresponding latent space matrix.')\n",
    "        del samples_filtered[index]\n",
    "        del sample_latents[index]\n",
    "        del tensor_shapes[index]\n",
    "\n",
    "sample_latents_np = np.array(sample_latents) # convert sample_latents to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1236, 176)\n"
     ]
    }
   ],
   "source": [
    "flattened_tensors = [tensor.flatten() for tensor in sample_latents]\n",
    "embeddings = np.vstack(flattened_tensors)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use t-SNE in order to narrow down to 2 the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = samples_filtered\n",
    "audio_names = remove_common_part(samples_filtered)\n",
    "\n",
    "dataset = fo.Dataset() # create a FiftyOne SampleCollection\n",
    "\n",
    "# create a sample for each audio file\n",
    "for audio_path, audio_name in zip(audio_paths, audio_names):\n",
    "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    if 'BD' in audio_name: label = 'BD' # label BD\n",
    "    elif 'SD' in audio_name: label = 'SD' # label SD\n",
    "    elif 'onset' in audio_name: label = 'Tap' # label SD\n",
    "    else: label = 'other' # label other\n",
    "    # create and append sample\n",
    "    sample = fo.Sample(filepath=audio_path, audio=audio_data, sample_rate=sample_rate, label=label, audio_name=audio_name)\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# dataset.save('../data') # save the the datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1236 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 1236 samples in 0.027s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1236\n",
      "[t-SNE] Computed conditional probabilities for sample 1236 / 1236\n",
      "[t-SNE] Mean sigma: 2.715692\n",
      "[t-SNE] Computed conditional probabilities in 0.028s\n",
      "[t-SNE] Iteration 50: error = 81.1890564, gradient norm = 0.1522968 (50 iterations in 0.134s)\n",
      "[t-SNE] Iteration 100: error = 82.5773010, gradient norm = 0.2936852 (50 iterations in 0.141s)\n",
      "[t-SNE] Iteration 150: error = 84.1498795, gradient norm = 0.2638909 (50 iterations in 0.117s)\n",
      "[t-SNE] Iteration 200: error = 85.7504425, gradient norm = 0.2561235 (50 iterations in 0.125s)\n",
      "[t-SNE] Iteration 250: error = 86.0628586, gradient norm = 0.2616243 (50 iterations in 0.138s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 86.062859\n",
      "[t-SNE] Iteration 300: error = 2.5489414, gradient norm = 0.0112634 (50 iterations in 0.138s)\n",
      "[t-SNE] Iteration 350: error = 2.3950076, gradient norm = 0.0030336 (50 iterations in 0.143s)\n",
      "[t-SNE] Iteration 400: error = 2.3240526, gradient norm = 0.0134905 (50 iterations in 0.130s)\n",
      "[t-SNE] Iteration 450: error = 2.2663217, gradient norm = 0.0014025 (50 iterations in 0.100s)\n",
      "[t-SNE] Iteration 500: error = 2.2583630, gradient norm = 0.0012264 (50 iterations in 0.118s)\n",
      "[t-SNE] Iteration 550: error = 2.2534924, gradient norm = 0.0009189 (50 iterations in 0.103s)\n",
      "[t-SNE] Iteration 600: error = 2.2502792, gradient norm = 0.0009277 (50 iterations in 0.101s)\n",
      "[t-SNE] Iteration 650: error = 2.2479463, gradient norm = 0.0006700 (50 iterations in 0.105s)\n",
      "[t-SNE] Iteration 700: error = 2.2461293, gradient norm = 0.0004489 (50 iterations in 0.097s)\n",
      "[t-SNE] Iteration 750: error = 2.2449489, gradient norm = 0.0005045 (50 iterations in 0.093s)\n",
      "[t-SNE] Iteration 800: error = 2.2438881, gradient norm = 0.0007867 (50 iterations in 0.109s)\n",
      "[t-SNE] Iteration 850: error = 2.2429824, gradient norm = 0.0003777 (50 iterations in 0.084s)\n",
      "[t-SNE] Iteration 900: error = 2.2421925, gradient norm = 0.0003203 (50 iterations in 0.085s)\n",
      "[t-SNE] Iteration 950: error = 2.2416453, gradient norm = 0.0003480 (50 iterations in 0.093s)\n",
      "[t-SNE] Iteration 1000: error = 2.2409256, gradient norm = 0.0003282 (50 iterations in 0.087s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 2.240926\n"
     ]
    }
   ],
   "source": [
    "# compute a 2D representation using t-SNE\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings,\n",
    "    num_dims=2,\n",
    "    method='tsne',\n",
    "    brain_key='mnist_test',\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "      <th>audio_name</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.999027</td>\n",
       "      <td>3.540467</td>\n",
       "      <td>BD</td>\n",
       "      <td>WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...</td>\n",
       "      <td>C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.950240</td>\n",
       "      <td>14.472720</td>\n",
       "      <td>BD</td>\n",
       "      <td>WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...</td>\n",
       "      <td>C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.673388</td>\n",
       "      <td>9.086163</td>\n",
       "      <td>BD</td>\n",
       "      <td>WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...</td>\n",
       "      <td>C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.443605</td>\n",
       "      <td>9.832090</td>\n",
       "      <td>BD</td>\n",
       "      <td>WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...</td>\n",
       "      <td>C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.109398</td>\n",
       "      <td>-2.461702</td>\n",
       "      <td>BD</td>\n",
       "      <td>WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...</td>\n",
       "      <td>C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y label  \\\n",
       "0 -6.999027   3.540467    BD   \n",
       "1 -8.950240  14.472720    BD   \n",
       "2  1.673388   9.086163    BD   \n",
       "3 -7.443605   9.832090    BD   \n",
       "4 -8.109398  -2.461702    BD   \n",
       "\n",
       "                                          audio_name  \\\n",
       "0  WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...   \n",
       "1  WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...   \n",
       "2  WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...   \n",
       "3  WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...   \n",
       "4  WAV/Individual Hits\\01. Bass Drum\\01. Clean\\02...   \n",
       "\n",
       "                                            filepath  \n",
       "0  C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...  \n",
       "1  C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...  \n",
       "2  C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...  \n",
       "3  C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...  \n",
       "4  C:\\Users\\asantos6\\Documents\\RAVERs\\data\\WAV\\In...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results.points, columns=['x', 'y'])\n",
    "df['label'] = dataset.values('label')\n",
    "df['audio_name'] = dataset.values('audio_name')\n",
    "df['filepath'] = dataset.values('filepath')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11ce9d7fa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create a scatter plot of the latent space\n",
    "data = []\n",
    "for label in df['label'].unique():\n",
    "    df_label = df[df['label'] == label]\n",
    "    scatter = go.Scatter(\n",
    "        x=df_label['x'], \n",
    "        y=df_label['y'], \n",
    "        mode='markers',\n",
    "        text=df_label['audio_name'],  # Add the audio name as text\n",
    "        hovertemplate='%{text}<extra></extra>',  # Customize the hover template\n",
    "        name=label,  # Use the label as the name of the trace\n",
    "        customdata=df_label['filepath']  # Add the file path as custom data\n",
    "    )\n",
    "    scatter.on_click(lambda x: print(x.points[0].hovertext))  # Print the name of the point when it's clicked\n",
    "    data.append(scatter)\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Adjust the margins (l, r, t, b stand for left, right, top, and bottom)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    margin=dict(l=24,r=24,b=24,t=24,pad=0)\n",
    ")\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id='scatter-plot', figure=fig),\n",
    "    html.Pre(id='click-data', style={'padding': '10px', 'color': 'white'}),\n",
    "    html.Audio(id='input-player', controls=True, autoPlay=True),\n",
    "    html.Audio(id='output-player', controls=True, autoPlay=False)\n",
    "])\n",
    "\n",
    "@app.callback(Output('click-data', 'children'), [Input('scatter-plot', 'clickData')])\n",
    "def display_click_data(clickData):\n",
    "    if clickData is None: return 'None'\n",
    "    absolute_path = clickData[\"points\"][0][\"customdata\"]\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "@app.callback(Output('input-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_input(clickData):\n",
    "    if clickData is None: return ''\n",
    "\n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "    with open(relative_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "    \n",
    "    return src\n",
    "\n",
    "@app.callback(Output('output-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_output(clickData):\n",
    "    if clickData is None: return ''\n",
    "    \n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "\n",
    "    audio, sr = read_audio(relative_path)\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(audio).reshape(1 ,1, -1)\n",
    "        z = model.encode(x)\n",
    "        x_hat = model.decode(z)\n",
    "    waveform_tensor = torch.squeeze(x_hat, 0)\n",
    "    output_path = '../output/output.wav'\n",
    "    torchaudio.save(output_path, waveform_tensor, sr)\n",
    "\n",
    "    with open(output_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "\n",
    "    return src\n",
    "\n",
    "if __name__ == '__main__': app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
