{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec63b55",
   "metadata": {},
   "source": [
    "# RAVE Latent Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5dddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "import rave\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3388b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tapped_dict = {\n",
    "    \"ac_dc\": \"../data/tapped/ac_dc-back_in_black-tapped.flac\",\n",
    "    \"amy_winehouse\": \"../data/tapped/Tears Dry On Their Own - Amy Whinehouse-tapped.flac\",\n",
    "    \"amália\": \"../data/tapped/Barco Negro (Mão Preta) - Amália Rodrigues-tapped.flac\",\n",
    "    \"arctic_monkeys\": \"../data/tapped/arctic_monkeys-the_view_from_the_afternoon-tapped.flac\",\n",
    "    \"beatles1\": \"../data/tapped/Come Together - The Beatles-tapped.flac\",\n",
    "    \"beatles2\": \"../data/tapped/the_beatles-in_my_life-tapped.flac\",\n",
    "    \"capitão_fausto\": \"../data/tapped/Morro na Praia - Capitão Fausto-tapped.flac\",\n",
    "    \"david_miguel\": \"../data/tapped/Inatel - David e Miguel-tapped.flac\",\n",
    "    \"gorillaz\": \"../data/tapped/Clint Eastwood - Gorillaz-tapped.flac\",\n",
    "    \"ibeyi\": \"../data/tapped/River - Ibeyi-tapped.flac\",\n",
    "    \"linda_martini\": \"../data/tapped/Febril - Linda Martini-tapped.flac\",\n",
    "    \"michael_jackson\": \"../data/tapped/Billie Jean - Michael Jackson-tapped.flac\",\n",
    "    \"qotsa\": \"../data/tapped/qotsa_no_one_knows-tapped.flac\",\n",
    "    \"queen\": \"../data/tapped/queen-another_one_bites_the_dust-tapped.flac\",\n",
    "    \"slowj\": \"../data/tapped/slowj-mundança-tapped.flac\",\n",
    "    \"strokes\": \"../data/tapped/Reptilia - The Strokes-tapped.flac\",\n",
    "    \"white_stripes\": \"../data/tapped/white_stripes-seven_nation_army-tapped.flac\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc68de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(file_path, trim_interval=None, mono=True, print_it=False):\n",
    "    audio, sr = librosa.load(file_path, mono=mono)\n",
    "    audio_dim = len(audio.shape)\n",
    "    if not mono and audio_dim == 1:\n",
    "        audio = np.asarray((audio, audio))\n",
    "    if trim_interval is not None:\n",
    "        ti = trim_interval[0]\n",
    "        tf = trim_interval[1]\n",
    "        audio = trim_audio(audio, sr, ti, tf, mono)\n",
    "    if print_it:\n",
    "        print(audio.shape)\n",
    "        print(sr)\n",
    "    return audio, sr\n",
    "\n",
    "def trim_audio(audio, sr, ti, tf, mono=True):\n",
    "    i = ti * sr\n",
    "    f = tf * sr\n",
    "    if mono:\n",
    "        t_audio = audio[i:f]\n",
    "    else:\n",
    "        t_audio = audio[:, i:f]\n",
    "    return t_audio\n",
    "\n",
    "\n",
    "def remix_audio(left_audio_array, right_audio_array, sample_rate=44100):\n",
    "    # Ensure both audio arrays have the same length\n",
    "    length = min(len(left_audio_array), len(right_audio_array))\n",
    "    left_audio_array = left_audio_array[:length]\n",
    "    right_audio_array = right_audio_array[:length]\n",
    "\n",
    "    # Create a stereo audio array with left channel and right channel\n",
    "    stereo_audio_array = np.column_stack((left_audio_array, right_audio_array))\n",
    "\n",
    "    # Save the remixed audio to the specified output path\n",
    "    #sf.write(output_path, stereo_audio_array, sample_rate)\n",
    "\n",
    "    return stereo_audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#song = \"ac_dc\"\n",
    "#song = \"amália\"\n",
    "#song = \"amy_winehouse\"\n",
    "#song = \"arctic_monkeys\"\n",
    "#song = \"beatles1\"\n",
    "#song = \"beatles2\"\n",
    "##song = \"capitão_fausto\"\n",
    "#song = \"david_miguel\"\n",
    "#song = \"gorillaz\"\n",
    "#song = \"ibeyi\"\n",
    "#song = \"linda_martini\"\n",
    "#song = \"michael_jackson\"\n",
    "#song = \"qotsa\"\n",
    "##song = \"queen\"\n",
    "song = \"slowj\"\n",
    "#song = \"strokes\"\n",
    "#song = \"white_stripes\"\n",
    "\n",
    "trim = True\n",
    "\n",
    "trim_interval = None\n",
    "# choose the interval in which you want to trim the audio (in seconds)\n",
    "if trim:\n",
    "    trim_interval = (15, 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda39500",
   "metadata": {},
   "outputs": [],
   "source": [
    "tapped_path = tapped_dict[song]\n",
    "#tapped_path = '../data/KickSequence.wav'\n",
    "\n",
    "tapped_audio, sr = read_audio(tapped_path, trim_interval)\n",
    "ipd.Audio(tapped_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'GMDrums_v3_29-09_3M_streaming'\n",
    "#model_name = 'percussion'\n",
    "#model_name = 'darbouka_onnx'\n",
    "model_name = 'drumkit_v1'\n",
    "model = torch.jit.load(f'../models/{model_name}.ts')\n",
    "\n",
    "desired_length = 44100\n",
    "\n",
    "# Calculate the amount of padding needed\n",
    "padding_width = max(0, desired_length - len(tapped_audio))\n",
    "print(len(tapped_audio))\n",
    "\n",
    "# Pad the array with zeros\n",
    "tapped_audio = np.pad(tapped_audio, (0, padding_width), mode='constant', constant_values=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(tapped_audio).reshape(1,1,-1)\n",
    "\n",
    "    # encode and decode the audio with RAVE\n",
    "    z = model.encode(x)\n",
    "    latent_space_matrix = torch.squeeze(z, 0)\n",
    "    #print(latent_space_matrix)\n",
    "\n",
    "\n",
    "    x_hat = model.decode(z)\n",
    "waveform_tensor = torch.squeeze(x_hat, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_distance(tensor1, tensor2, distance_metric='euclidean'):\n",
    "    if distance_metric == 'euclidean':\n",
    "        return F.pairwise_distance(tensor1, tensor2)\n",
    "    elif distance_metric == 'cosine':\n",
    "        return 1 - F.cosine_similarity(tensor1, tensor2)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "desired_length = 44100\n",
    "\n",
    "# Calculate the amount of padding needed\n",
    "padding_width = max(0, desired_length - len(tapped_audio))\n",
    "\n",
    "# Pad the array with zeros\n",
    "tapped_audio = np.pad(tapped_audio, (0, padding_width), mode='constant', constant_values=0)\n",
    "\n",
    "# Example usage\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(tapped_audio).reshape(1,1,-1)\n",
    "\n",
    "    # encode and decode the audio with RAVE\n",
    "    z = model.encode(x)\n",
    "    tapped_latent_space_matrix = torch.squeeze(z, 0)\n",
    "    \n",
    "kick_path = '../data/WAV/Individual Hits/01. Bass Drum/04. Color/Color 04/03. Long/BD 909 Color 04 Long B 01.wav'\n",
    "\n",
    "kick_audio, sr = read_audio(kick_path)\n",
    "\n",
    "# Calculate the amount of padding needed\n",
    "padding_width = max(0, desired_length - len(kick_audio))\n",
    "\n",
    "# Pad the array with zeros\n",
    "padded_kick_audio = np.pad(kick_audio, (0, padding_width), mode='constant', constant_values=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.from_numpy(padded_kick_audio).reshape(1,1,-1)\n",
    "\n",
    "    # encode and decode the audio with RAVE\n",
    "    z = model.encode(x)\n",
    "    kick_latent_space_matrix = torch.squeeze(z, 0)\n",
    "\n",
    "distance = calculate_distance(tapped_latent_space_matrix, kick_latent_space_matrix, distance_metric='euclidean')\n",
    "#print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f9bc8-1256-466a-8805-97883429ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(padded_kick_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530215e-59e5-4c39-b52e-b40a3e3f5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(tapped_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_tensors(tensor1, tensor2, alpha):\n",
    "    return alpha * tensor1 + (1 - alpha) * tensor2\n",
    "\n",
    "# Example usage\n",
    "alpha_value = 0.5  # Adjust alpha as needed\n",
    "interpolated_tensor = interpolate_tensors(kick_latent_space_matrix, tapped_latent_space_matrix, alpha_value)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    z = torch.unsqueeze(interpolated_tensor, 0)\n",
    "    print(interpolated_tensor.shape)\n",
    "\n",
    "\n",
    "    x_hat = model.decode(z)\n",
    "waveform_tensor = torch.squeeze(x_hat, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output.wav'\n",
    "torchaudio.save(output_file, waveform_tensor, sr)\n",
    "\n",
    "interpolated_audio, sr = read_audio(output_file)\n",
    "print(interpolated_audio.shape)\n",
    "\n",
    "ipd.Audio(interpolated_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_files_with_keyword(folder_path, keyword):\n",
    "    matching_files = []\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if the file is a regular file and contains the keyword\n",
    "        if os.path.isfile(file_path) and keyword in filename:\n",
    "            matching_files.append(file_path)\n",
    "\n",
    "    return sorted(matching_files)\n",
    "\n",
    "# Replace 'your_folder_path' with the path to the folder you want to search\n",
    "folder_path = '../data/Roland TR-909'\n",
    "keyword = 'Kick'\n",
    "\n",
    "kick_files = find_files_with_keyword(folder_path, keyword)\n",
    "\n",
    "kick_files.append('../data/Roland TR-909/TR-909Crash.wav')\n",
    "\n",
    "# Print the list of matching files\n",
    "#print(f\"Files in '{folder_path}' containing the keyword '{keyword}':\")\n",
    "#for file_path in matching_files:\n",
    "#    print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def find_wav_files(folder_path):\n",
    "    wav_files = []\n",
    "    \n",
    "    # Use os.walk to traverse the directory tree\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Use glob to find .wav files in the current directory\n",
    "        wav_files.extend(glob.glob(os.path.join(root, '*.wav')))\n",
    "    \n",
    "    return wav_files\n",
    "\n",
    "# Example usage\n",
    "folder_path = '../data/WAV/Individual Hits'\n",
    "wav_files = find_wav_files(folder_path)\n",
    "\n",
    "print(\"Found .wav files:\")\n",
    "#for wav_file in wav_files:\n",
    "    #print(wav_file)\n",
    "print(len(wav_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b14ff-b6e2-4f24-85ea-3d83776da51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    z = torch.unsqueeze(tapped_latent_space_matrix, 0) \n",
    "    x_hat = model.decode(z)\n",
    "tapped_rave_original = torch.squeeze(x_hat, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e77a0-2eec-4ae8-a6e6-7c668dfe6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(tapped_rave_original, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799e165-4ca3-4f60-9ac2-4dc0e65dbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    z = torch.unsqueeze(kick_latent_space_matrix, 0) \n",
    "    x_hat = model.decode(z)\n",
    "kick_rave_original = torch.squeeze(x_hat, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0884792-4661-4172-85a7-c6fa381c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(kick_rave_original, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b8cd4-7edb-43f7-9f03-526dcaa62998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def interpolate_one_dimension(point1, point2, t, dim):\n",
    "    result = np.copy(point1)\n",
    "    result[:, dim] = (1 - t) * point1[:, dim] + t * point2[:, dim]\n",
    "    return result\n",
    "\n",
    "# Example with random points in a 16x22 latent space\n",
    "latent_point1 = tapped_latent_space_matrix\n",
    "latent_point2 = kick_latent_space_matrix\n",
    "\n",
    "# Choose the dimension to interpolate (e.g., the first dimension, dim=0)\n",
    "interpolated_dimension = 0\n",
    "\n",
    "# Interpolate with t in the range [0, 1]\n",
    "t_values = np.linspace(0, 1, num=10)\n",
    "interpolated_points = [interpolate_one_dimension(latent_point1, latent_point2, t, interpolated_dimension) for t in t_values]\n",
    "\n",
    "# Visualization (assuming 16x22 latent space)\n",
    "plt.plot(latent_point1[:, interpolated_dimension], label='Point 1')\n",
    "plt.plot(latent_point2[:, interpolated_dimension], label='Point 2')\n",
    "for i, t in enumerate(t_values):\n",
    "    plt.plot(interpolated_points[i][:, interpolated_dimension], label=f'Interpolated {t:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Linear Interpolation along Dimension {interpolated_dimension}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1cac9-6867-42ef-987f-8f6ddd48c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_audios=[]\n",
    "for i in interpolated_points:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        z = torch.unsqueeze(interpolated_tensor, 0)      \n",
    "        x_hat = model.decode(z)\n",
    "    interpolated_audios.append(torch.squeeze(x_hat, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4993d6-f7c1-4817-9f19-292b2ea5df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output.wav'\n",
    "\n",
    "torchaudio.save(output_file, interpolated_audios[9], sr)\n",
    "\n",
    "interpolated_audio, sr = read_audio(output_file)\n",
    "print(interpolated_audio.shape)\n",
    "\n",
    "ipd.Audio(interpolated_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_latents = []\n",
    "kick_filtered_files = []\n",
    "for kick_path in wav_files:\n",
    "\n",
    "    kick_audio, sr = read_audio(kick_path, print_it=False)\n",
    "    #kick_audio = np.pad()\n",
    "    #kick_audio = kick_audio[:1500]\n",
    "    if kick_audio.shape[0] < 4410:\n",
    "        continue\n",
    "    \n",
    "    kick_filtered_files.append(kick_path)\n",
    "    desired_length = 44100\n",
    "\n",
    "    # Calculate the amount of padding needed\n",
    "    padding_width = max(0, desired_length - len(kick_audio))\n",
    "\n",
    "    # Pad the array with zeros\n",
    "    kick_audio = np.pad(kick_audio, (0, padding_width), mode='constant', constant_values=0)\n",
    "    #print(kick_audio.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(kick_audio).reshape(1,1,-1)\n",
    "\n",
    "        # encode and decode the audio with RAVE\n",
    "        z = model.encode(x)\n",
    "        kick_latent_space_matrix = torch.squeeze(z, 0)\n",
    "        kick_latents.append(kick_latent_space_matrix)\n",
    "\n",
    "# Check the shapes of all tensors\n",
    "\n",
    "tensor_shapes = [tensor.shape for tensor in kick_latents]\n",
    "if all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    print(\"All have same shape\")\n",
    "print(tensor_shapes)\n",
    "\n",
    "kick_latents.append(tapped_latent_space_matrix)\n",
    "kick_latents.extend(interpolated_points)\n",
    "\n",
    "\n",
    "# Ensure all tensors have the same shape\n",
    "if not all(shape == tensor_shapes[0] for shape in tensor_shapes):\n",
    "    raise ValueError(\"All input tensors must have the same shape.\")\n",
    "\n",
    "# Convert each PyTorch tensor to a NumPy array\n",
    "#latent_space_numpy = np.stack([tensor.numpy() for tensor in kick_latents], axis=0)\n",
    "latent_space_numpy = np.array(kick_latents)\n",
    "#print(latent_space_numpy)\n",
    "\n",
    "# Now, latent_space_numpy is a NumPy array that you can use as input to UMAP\n",
    "#kick_tensor = torch.cat(kick_latents, axis=1)\n",
    "#kick_tensor = kick_tensor.permute(-1, 0)\n",
    "#print(kick_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13397376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Reshape each tensor to a 1D array while preserving temporal order\n",
    "flattened_tensors = [tensor.flatten() for tensor in kick_latents]\n",
    "\n",
    "# Stack the flattened tensors into a 2D array\n",
    "data_matrix = np.vstack(flattened_tensors)\n",
    "print(data_matrix.shape)\n",
    "\n",
    "# Apply UMAP\n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components)\n",
    "umap_result = umap_model.fit_transform(data_matrix)\n",
    "\n",
    "print(umap_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57765be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_part(file_names):\n",
    "    # Find the common prefix\n",
    "    common_prefix = os.path.commonprefix(file_names)\n",
    "    \n",
    "    # Find the common suffix (if needed)\n",
    "    common_suffix = os.path.commonprefix([name[::-1] for name in file_names])[::-1]\n",
    "\n",
    "    # Remove the common part from each file name\n",
    "    modified_file_names = [name[len(common_prefix):] for name in file_names]\n",
    "\n",
    "    return modified_file_names\n",
    "\n",
    "\n",
    "modified_file_names = remove_common_part(kick_filtered_files)\n",
    "modified_file_names.append(tapped_path)\n",
    "kick_filtered_files.append(tapped_path)\n",
    "\n",
    "for i in interpolated_points:\n",
    "    modified_file_names.append(tapped_path)\n",
    "    kick_filtered_files.append(tapped_path)\n",
    "\n",
    "#print(\"Modified file names:\", modified_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap.plot\n",
    "#umap.plot.points(umap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66096f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the UMAP result\n",
    "for i, file_name in enumerate(modified_file_names):\n",
    "    if \"BD\" in file_name:\n",
    "        color = 'red'\n",
    "    elif \"SD\" in file_name:\n",
    "        color = 'green'\n",
    "    elif \"tapped\" in file_name:\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    plt.scatter(umap_result[i, 0], umap_result[i, 1], c=color, marker='o', s=10)\n",
    "print(umap_result.shape)\n",
    "print(len(modified_file_names))\n",
    "    \n",
    "plt.title('UMAP Visualization of Latent Space')\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output.wav'\n",
    "torchaudio.save(output_file, waveform_tensor, sr)\n",
    "\n",
    "rave_audio, sr = read_audio(output_file)\n",
    "print(rave_audio.shape)\n",
    "\n",
    "ipd.Audio(rave_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'ipywidgets>=8,<9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import librosa\n",
    "\n",
    "# Example list of audio file paths\n",
    "audio_files = kick_filtered_files\n",
    "audio_names = modified_file_names\n",
    "\n",
    "# Create a FiftyOne SampleCollection\n",
    "dataset = fo.Dataset()\n",
    "\n",
    "# Iterate over audio files and create samples\n",
    "for audio_file, audio_name in zip(audio_files, audio_names):\n",
    "    # Load audio data using librosa or your preferred library\n",
    "    audio_data, sample_rate = librosa.load(audio_file, sr=None)  # Adjust parameters as needed\n",
    "\n",
    "    if \"BD\" in audio_name: label = 'BD'\n",
    "    elif \"SD\" in audio_name: label = 'SD'\n",
    "    elif \"mundança\" in audio_name: label = 'tap'\n",
    "    else: label = 'other'\n",
    "    \n",
    "    # Create a FiftyOne Sample\n",
    "    sample = fo.Sample(\n",
    "        filepath=audio_file,\n",
    "        id=audio_name,\n",
    "        audio=audio_data,\n",
    "        sample_rate=sample_rate,\n",
    "        label=label,\n",
    "        audio_name=audio_name,  # Add the audio name as a custom field\n",
    "    )\n",
    "\n",
    "    # Add the sample to the dataset\n",
    "    dataset.add_sample(sample)\n",
    "\n",
    "# Save the SampleCollection (optional)\n",
    "#dataset.save(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d941dd8-a46b-4bba-bbcb-a4838c490326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fiftyone as fo\n",
    "#import fiftyone.core.fields as fof\n",
    "#import numpy as np\n",
    "\n",
    "# Assume you have a latent tensor\n",
    "#extra_latent_tensor = tapped_latent_space_matrix\n",
    "#print(tapped_latent_space_matrix.shape)\n",
    "\n",
    "# Create a new sample with the latent tensor\n",
    "#sample = fo.Sample(filepath=tapped_path, label=\"input\")\n",
    "\n",
    "# Add the array field to the sample\n",
    "#sample[\"latent_tensor\"] = fof.ArrayField()\n",
    "\n",
    "# Assign the latent tensor to the array field\n",
    "#sample[\"latent_tensor\"].value = extra_latent_tensor\n",
    "\n",
    "# Add the sample to the dataset\n",
    "#dataset.add_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "# Construct a ``num_samples x num_pixels`` array of images\n",
    "embeddings = data_matrix\n",
    "\n",
    "# Compute 2D representation\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings,\n",
    "    num_dims=2,\n",
    "    method=\"tsne\",\n",
    "    brain_key=\"mnist_test\",\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe255be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(results))\n",
    "print(results.points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a18408-96a8-4201-a8dd-192d01fd123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.visualize(labels=\"label\")\n",
    "plot.show(height=720)\n",
    "\n",
    "session.plots.attach(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad1516-17f2-4aa0-bd1b-dd4680b14a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "latent_space_pca = pca.fit_transform(latent_space_matrix)\n",
    "print(latent_space_pca)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "plt.plot(cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Print the principal components\n",
    "print('Principal Components:')\n",
    "print(pca.components_.shape)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb15ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "latent_space_2d = pca.fit_transform(latent_space_matrix)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(latent_space_2d[:, 0], latent_space_2d[:, 1])\n",
    "plt.title('Latent Space Matrix in 2D')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(tapped_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c782be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remixed_audio = remix_audio(tapped_audio, rave_audio, sample_rate=sr)\n",
    "\n",
    "normalized_audio = remixed_audio / np.max(np.abs(remixed_audio))\n",
    "\n",
    "# Convert to 16-bit PCM format\n",
    "pcm_audio = (normalized_audio * 32767).astype(np.int16)\n",
    "\n",
    "# Save the PCM audio data to a temporary WAV file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_wav:\n",
    "    sf.write(temp_wav.name, pcm_audio, sr)\n",
    "\n",
    "# Display the audio using ipd.Audio\n",
    "ipd.display(ipd.Audio(temp_wav.name))\n",
    "\n",
    "# Clean up the temporary file\n",
    "os.remove(temp_wav.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import correlate\n",
    "\n",
    "# Assuming rave_audio and tapped_audio are your stereo audio data with shape (length, 2)\n",
    "# Replace them with your actual stereo audio data\n",
    "\n",
    "# Choose one channel for onset detection (e.g., left channel)\n",
    "rave_channel = rave_audio\n",
    "tapped_channel = tapped_audio\n",
    "\n",
    "# Normalize the audio to the range [-1, 1]\n",
    "rave_channel_normalized = rave_channel / np.max(np.abs(rave_channel))\n",
    "tapped_channel_normalized = tapped_channel / np.max(np.abs(tapped_channel))\n",
    "\n",
    "# Compute onsets using librosa onset detection\n",
    "rave_onsets = librosa.onset.onset_detect(rave_channel_normalized, sr=sr)\n",
    "tapped_onsets = librosa.onset.onset_detect(tapped_channel_normalized, sr=sr)\n",
    "\n",
    "# Compute cross-correlation\n",
    "cross_corr = correlate(rave_channel_normalized, tapped_channel_normalized, mode='full')\n",
    "\n",
    "# Find the time lag (shift) with maximum cross-correlation\n",
    "time_lag = np.argmax(cross_corr) - len(rave_channel_normalized) + 1\n",
    "\n",
    "# Shift the tapped_audio to align with rave_audio\n",
    "aligned_tapped_audio = np.roll(tapped_audio, time_lag, axis=0)\n",
    "\n",
    "# Now, aligned_tapped_audio and rave_audio are aligned based on onset detection\n",
    "\n",
    "# Display the aligned audio using ipd.Audio\n",
    "ipd.display(ipd.Audio(aligned_tapped_audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 2D representation\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings,\n",
    "    num_dims=2,\n",
    "    method=\"tsne\",\n",
    "    brain_key=\"mnist_test\",\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results.points, columns=['x', 'y'])\n",
    "df['label'] = dataset.values('label')\n",
    "df['audio_name'] = dataset.values('audio_name')\n",
    "df['filepath'] = dataset.values('filepath')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create a scatter plot of the latent space\n",
    "data = []\n",
    "for label in df['label'].unique():\n",
    "    df_label = df[df['label'] == label]\n",
    "    scatter = go.Scatter(\n",
    "        x=df_label['x'], \n",
    "        y=df_label['y'], \n",
    "        mode='markers',\n",
    "        text=df_label['audio_name'],  # Add the audio name as text\n",
    "        hovertemplate='%{text}<extra></extra>',  # Customize the hover template\n",
    "        name=label,  # Use the label as the name of the trace\n",
    "        customdata=df_label['filepath']  # Add the file path as custom data\n",
    "    )\n",
    "    scatter.on_click(lambda x: print(x.points[0].hovertext))  # Print the name of the point when it's clicked\n",
    "    data.append(scatter)\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Adjust the margins (l, r, t, b stand for left, right, top, and bottom)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    margin=dict(l=24,r=24,b=24,t=24,pad=0)\n",
    ")\n",
    "\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def get_relative_path(absolute_path):\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id='scatter-plot', figure=fig),\n",
    "    html.Pre(id='click-data', style={'padding': '10px', 'color': 'white'}),\n",
    "    html.Audio(id='input-player', controls=True, autoPlay=True),\n",
    "    html.Audio(id='output-player', controls=True, autoPlay=False)\n",
    "])\n",
    "\n",
    "@app.callback(Output('click-data', 'children'), [Input('scatter-plot', 'clickData')])\n",
    "def display_click_data(clickData):\n",
    "    if clickData is None: return 'None'\n",
    "    absolute_path = clickData[\"points\"][0][\"customdata\"]\n",
    "    base_path = os.getcwd()\n",
    "    return os.path.relpath(absolute_path, base_path)\n",
    "\n",
    "@app.callback(Output('input-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_input(clickData):\n",
    "    if clickData is None: return ''\n",
    "\n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "    with open(relative_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "    \n",
    "    return src\n",
    "\n",
    "@app.callback(Output('output-player', 'src'), [Input('scatter-plot', 'clickData')])\n",
    "def play_output(clickData):\n",
    "    if clickData is None: return ''\n",
    "    \n",
    "    relative_path = get_relative_path(clickData[\"points\"][0][\"customdata\"])\n",
    "\n",
    "    audio, sr = read_audio(relative_path)\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(audio).reshape(1 ,1, -1)\n",
    "        z = model.encode(x)\n",
    "        x_hat = model.decode(z)\n",
    "    waveform_tensor = torch.squeeze(x_hat, 0)\n",
    "    output_path = '../output/output.wav'\n",
    "    torchaudio.save(output_path, waveform_tensor, sr)\n",
    "\n",
    "    with open(output_path, 'rb') as audio_file:\n",
    "        encoded_audio = base64.b64encode(audio_file.read()).decode('ascii')\n",
    "    src = f'data:audio/mp3;base64,{encoded_audio}'\n",
    "\n",
    "    return src\n",
    "\n",
    "if __name__ == '__main__': app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raversenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
